# BA结合神经网络

## BA-NET

- [论文](https://arxiv.org/abs/1806.04807)
- [git](https://github.com/frobelbest/BANet)

### 摘要

利用feature-metric(结合了特征点和度量信息的)BA创建的一种网络结构来处理SFM问题，

它们用feature-metric error的方式来实现multi-view geometry多视图几何约束。

### 主要工作

![image-20230708165631026](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/202307081656176.png)

<center style="color:#C125C0C0">BA-NET整体结构</center>

- 基本流程：
  1. 首先，DRN-54提取feature maps
  2. 然后，同时两件事：
     - Basis Depth Maps Generator对第一张图$I_1$生成多个基准深度图basis depth maps
     - Feature Pyramid Constructor对1中的feature maps提取出特征F 
  3. 最后，BA层用下面的1式同时优化相机位姿，深度

1. **特征金字塔结构**：

   ![image-20230708170036977](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/202307081700007.png)

   - 将骨干网络backbone network(蓝色)DRN-54(Dilated Residual Network-54)的最后几个残差快标记为C1,C2,C3,C4
     - 残差块（residual block）是深度残差网络（Residual Network）的基本组成单元。它包含一系列卷积层、激活函数和跳跃连接（skip connection）。
     - 跳跃连接将输入特征与残差块的输出特征进行直接相加，以便在信息传递过程中保留更多的低层细节和梯度信号。
     - 这些残差块通常是网络中的最后几个阶段，用于提取更高级别的特征表示，以便进行更复杂的任务，如语义理解或图像分割。
     - TIPS:本文用的DRN-54因为效率问题用ordinary convolution替代了dilation convolution。
   - 用Bilinear interpolation（双线性插值）对特征图$C^{k+1}$进行上采样，并和$C^k$结合后，用3X3的卷积层来将低维度到128维，最终得到图片$I_i$的特征$F_i$

2. **基准深度图**

  - Encoder: 用DRN-54来提取有用的特征
  - Decoder: 用卷积特征图作为基准深度图来优化

3. **BA层**：

   ![image-20230708183312273](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/202307081833302.png)

   - BA优化的error：
     $$
     e^f_{i,j}(\mathcal{X})=F_i(\pi(\mathbf{T}_i,d_j\cdot\mathbf{q}_j))-F_1(\mathbf{q_j})\tag{1}
     $$
     - $F_i$：是照片$I_i$的的特征金字塔
       - 特征金字塔：通过构建多尺度的特征表示，以捕捉图像中不同尺度的物体信息
     - $\mathcal{X}$：参数
       - $T_i$：i图下的相机位姿
       - $d_j$：j观测点(特征点)的深度
       - $q_j$：j观测点(特征点)的坐标
     
   - Figure4:

     - $E(\mathcal{X})=[e_{1,1}^f(\mathcal{X}),e_{1,2}^f(\mathcal{X})\cdots e_{N_i,N_j}^f(\mathcal{X})]$

       计算所有的$N_i$照片和$N_j$像素点的误差

     - 用global average pooling全局平均池化来聚合每个特征通道上所有像素的E(X)绝对值。

       然后用MLP网络预测damping factor$\lambda$。

     - 最后更新$\mathcal{X}$
       $$
       \begin{align}
       \Delta\mathcal{X}&=(J(\mathcal{X})^TJ(\mathcal{X})+\lambda D(\mathcal{X}))^{-1}J(\mathcal{X})^{T}E(\mathcal{X})
       \\
       \Delta\mathcal{X}&=g(\mathcal{X};\mathbb{F})
       \\
       \mathcal{X}_k&=g(\mathcal{X}_{k-1};\mathbb{F})\circ \mathcal{X}_{k-1}
       \end{align}
       $$

### 总结

网络模型 BA-NET利用特征度量误差(feature-metric error)来强制执行多视角几何约束。然后由feature-metric BA同时优化深度和位姿。

整个BA-Net的流程是可微分的，所以模型可以从数据中学习特征表示F，并且通过训练过程中的反向传播来更新模型参数$\lambda$

## DeepSFM

- [论文](https://arxiv.org/abs/1912.09697)
- [git](https://github.com/weixk2015/DeepSFM)

### 摘要

提出了一个名为DeepSFM的物理驱动架构，将深度学习和**显式的结构约束**(3D cost volume)相结合，来解决三维重建问题。

DeepSFM由两个基于成本体素的架构组成，分别用于深度估计和姿态估计，通过迭代运行来改进两者。

### 主要工作

![image-20230710000609230](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/202307100006281.png)

- 基本流程
  1. 使用2D CNN对输入图像进行处理，提取图像中的**光度特征**(photometric feature)
     - 光度特征可以是灰度图像中的像素值，也可以是彩色图像中的颜色通道值。
  2. 利用光度特征构建**成本体素**cost volume。
     - 成本体素是一种表示源图像和目标图像之间匹配程度的数据结构。
     - 它通过比较光度特征的相似性来计算匹配代价
  3. 初始源深度图(source depth maps)和相机位姿被用来引入**光度和几何一致性**(consistency)
     - 这些一致性约束有助于提高深度估计和姿态估计的准确性。
  4. 应用一系列的3D CNN层来处理**深度成本体素**（D-CV）和**位姿成本体素**（P-CV）。
     - 这些层用于进一步提取和学习特征，以优化深度和姿态的估计。
  5. 对于深度图，使用**上下文网络**(context network)和**深度回归操作**(depth regression operation)来生成目标图像的预测深度图。
     - 上下文网络可以考虑图像中的上下文信息
     - 深度回归操作用于预测每个像素点的深度值。

- **深度成本体素**(D-CV)

  区别于以前的成本体素，D-CV进一步利用由深度图带来的局部几何一致性约束(local geometric consistency constraints)。

  假设采样Hypothesis Sampling：

  - 为了将源视点的特征和深度图从源视角(source viewpoint)反投影(back-project)到目标视点的三维空间中，在逆深度空间(inverse-depth space)中均匀采样了一组L个**虚拟深度图平面**($d_l$)，
  - 这些平面与目标视点的前向方向（z轴）垂直。
  - 这些平面被用作输出深度图的假设，并且可以在它们之上构建成本体素。

  D-CV由三部分组成：

  - 目标图像特征target image features

  - 变换的源图特征warped source image features

    - 利用内参K和位姿T，将原图特征 $F$ 变换(warp)到每一个假设的深度图平面(hypothetical depth map planes $d_l$)
    - 再结合目标图像特征，得到一个$2Channel\times L\times Width \times Height$的特征量feature volume
      - 特征F的size: $Channel\times Width \times Height$
      - L：虚拟平面个数

  - 齐次深度一致性图homogeneous depth consistency maps

    为了利用几何一致性并提升深度预测的质量，我们在每个虚拟平面上增加了另外两个通道：

    1. 源视角的变换初始深度图(warped initial depth maps from the source views)

       和上面的变换的源图特征相同

    2. 以源视角为参考的投影虚拟深度平面(the projected virtual depth plane from the perspective of the source view)

       需要从目标视角到源视角进行坐标转换

- **位姿成本体素**(P-CV)

  ![image-20230710021541970](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/202307100215004.png)

  <center style="color:#C125C0C0">假设位姿采样</center>

  区别于D-CV是在假设深度图平面上创建，P-CV是在假设位姿上建立的，假设位姿采样如上图所示。a是对平移采样，b是对旋转采样。采样后就得到一组虚拟的位姿。

  P-CV也由三部分组成：

  - 目标图像特征
  - 变换的源图像特征
    - 通过双线性插值(bilinear sampling)对源视角的特征进行变换，得到变换后的源特征图
    - 结合目标图像特征和深度图，得到一个$(2Channel+2)\times P\times Width \times Height$的4D cost volume
      - W,H：特征图的宽和高
      - channel：通道数
      - P：采样的位姿个数
  - 齐次深度一致性图
    - 将初始目标视角深度(initial target view depth)和源视角(source view depth)深度转换为一个齐次坐标系
      - 这样可以在成本体素构建过程中增强 相机姿态 和 多视角深度图 之间的几何一致性


### 总结

DeepSFM深度学习框架通过两个关键组件D-CV和P-CV实现在深度网络中显式地强制执行光度一致性、几何一致性和相机运动约束。

可以看作是一种增强的基于学习的BA算法，它充分发挥了可学习的先验知识和几何规则的优势。



## Salient BA for VSLAM

- [论文](https://arxiv.org/pdf/2012.11863.pdf)

### 摘要

旨在模仿人类视觉系统在不同任务下从自然场景中选择最显著和感兴趣的区域或点进行进一步处理。

用显著性预测模型(saliency prediction model)预测显著性地图(saliency map)，这个地图可以捕捉场景的语义的几何信息(scene semantic and geometric information)，然后这个地图的值作为传统BA中特征点的权重。

### 主要工作

![image-20230711005442714](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/202307110054860.png)

<center style="color:#C125C0C0">红色为本文贡献</center>

- 主要贡献：

  1. 提出了一个适用于室内和室外环境的 **Salient SLAM框架**，可以应用于各种应用领域。

     ![image-20230711010108808](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/202307110112364.png)

     <center style="color:#C125C0C0">Pipeline of visual saliency map prediction</center>

  2. 提出了一种新方法，基于KITTI生成**显著性数据集**Salient-KITTI(网上没找到)

     - 这个salient dataset包含几何和语义信息，用语义凝视(semantic gaze)来代替人眼凝视真值(human gaze ground-truth)
     - 之所以用这个数据集来训练，是因为之前依靠人眼追踪的gaze data无法捕捉到所有重要的信息。

  3. 用Salient-KITTI来做**显著性预测**

     1. 首先，对每个图像提取几何信息，如特征点、线条和平面
     2. 然后，使用SDC-Net在感兴趣物体周围生成分割掩码(segmented mask)
        - 作者选了13个类别（红绿灯、交通标志、道路、建筑物、人行道、停车场、铁轨、栅栏、桥梁、电线杆、电线杆组、植被、地形）来筛选几何信息
        - 动态区域中的特征不会出现在我们的显著性数据集中，如移动车辆、行人等
     3. 最后，基于我们提出的显著性数据集，使用DI-Net 获得一个显著性模型，并将其用于预测初始的显著性地图

  4. 提出了一种显著性**Salient Bundle Adjustment（SBA）**方法，以模拟人类视觉系统

     参考上面的总体流程图，SBA有两种：

     1. Motion-only BA: 用于优化位姿(R,t)，并最小化重映射误差
        $$
        \{R,t\}=\mathop{arg\ min}_{R,t}\sum_{i\in\mathcal{X}}\rho\big(w_i||x^i_{(\cdot)}-\pi_{(\cdot)}(\mathbf{RX})^i+t||^2_{\Sigma}\big)
        $$

        - $\rho$：robust Huber cost function

        - $w_i$:显著性权重salient weight
          $$
          w_i=aS^2(x_i,y_i)+b
          $$

          - S是显著性图Saliency map的像素值

     2. Local BA：用于优化共可见关键帧co-visible keyframe $\mathcal{K}_L$和这些关键帧中所有的地图点map-points $\mathcal{P}_L$
        $$
        \begin{align}
        &\{\mathbf{X}^i,\mathbf{R}_l,\mathbf{t}_l|i\in\mathcal{P_L},l\in\mathcal{K}_L \}=\mathop{arg\ min}_{\mathbf{X}^i,\mathbf{R}_l,\mathbf{t}_l}\sum_{k\in\mathcal{K}_L\cup\mathcal{K}_F}\sum_{j\in\mathcal{X}_k}\rho(E_{k,j})\\
        &E_{k,j}=w_i||x^j_{(\cdot)}-\pi_{(\cdot)}(\mathbf{R}_k\mathbf{X}^j+t_k)||^2_{\Sigma}
        \end{align}
        $$

        - X：地图点map-points $\mathcal{P}_L$和关键帧$\mathcal{K}_L$中key points相匹配的集合



### 总结

基于显著性信息，可以提高准确性和稳健性

## Semantic Scene Labeling-improve BA

- [论文](https://link.springer.com/chapter/10.1007/978-3-319-49409-8_13)

### 摘要

基于 CNN 的场景标记来几何约束捆绑调整，达到使用深度学习来改进城市规模的 SLAM的目的。

### 主要工作

主要思想是充分利用3d building建筑 models

用L-M优化的目标函数：
$$
\mathop{arg\ min}_\mathbf{X}\ \frac{1}{t-B(\mathbf{X})}+\sum_{q\in Q}W_qd(q,N_q) \tag{1}
$$

- $\mathbf{X}$:相机位姿和3d点的坐标

- Q：是被标记为建筑的3d点

  - 由CNN网络确定

- $d(\cdot)$：表示squared Euclidian distance

- $N_q$：离$q\in Q$最近的建筑平面

- $W_q=\mathcal{D}(P_z)$：权重
  $$
  P_Z=\frac{1}{N_c}\sum_{i=1}^nP_i\tag{2}
  $$

  - 3d点Z可有由n个观测值

  - $\mathcal{D}$:Dirichlet density function

    用于过滤较差的分割结果

  - $P_i$：3d点Z的一个可能分布probability distributions

  - $P_Z$：是所有可能分布的均值

- B：the sum of squared reprojection errors

### 总结

被证明比单纯的arg max更有效。但计算复杂度高，很难应用于实时SLAM

# 语义BA无DL

## Semantic Photometric BA

- [论文](https://arxiv.org/pdf/1712.00110.pdf)

### 摘要

语义光度BA，用深度学习获得3D object prior结合PBA，实现在自然图像序列(natural sequence of images)中实现优秀的对象重建效果

- 自然图像序列(natural sequence of images)：指在现实世界中以连续的时间间隔拍摄或捕获的图像序列
- 3D对象先验（3D object prior）：指对三维对象的先验知识或假设。根据以往的经验、统计数据或领域知识来建模对象的一般性质和特征。

### 主要工作

![image-20230712094913344](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/202307120949602.png)

<center style="color:#C125C0C0">Pipeline of the optimization</center>

- 基本流程

  物体的分类假设已知

  1. 用offline PBA pipeline对自然图像序列处理后，提供粗糙的深度图估计。并初始化相机位姿和style vector
  2. 优化目标：光度一致性photometric consistency$\mathcal{L}_{ph}$，轮廓匹配误差silhouette matching error$\mathcal{L}_{cd}$，和逆深度误差inverse depth error$\mathcal{L}_{invd}$

- 优化目标

  - 光度一致性photometric consistency$\mathcal{L}_{ph}$
    $$
    \begin{align}
    &\mathcal{L}_{ph}(\mathbf{p}_0,\{\Delta\mathbf{p}_l\}^{L-1}_{l=1},s)=\\
    &\sum_{l=1}^{L-1}\Big[\sum_{j=1}^{M_{\mathbf{p}_0}}\mathcal{L}_{\delta_1}(\mathcal{I}_0(\pi(\mathbf{x}_j;\mathbf{p}_0))-\mathcal{I}_l(\pi(\mathbf{x}_j;\mathbf{p}_0\circ\Delta\mathbf{p}_l)))\\
    &+\sum_{k=1}^{M_{\mathbf{p}_l}}\mathcal{L}_{\delta_1}(\mathcal{I}_l(\pi(\mathbf{x}_k;\mathbf{p}_0\circ\Delta\mathbf{p}_l))-\mathcal{I}_0(\pi(\mathbf{x}_k;\mathbf{p}_0))\Big]
    \end{align} \tag{1}
    $$

    - $\mathcal{L}_\delta(\cdot)$: Huber loss
    - $\mathbf{p}_0$: the global camera pose of the target frame
    - $\Delta\mathbf{p}_l$:the relative camera pose between each source frame and the corresponding target frame
    - $\mathcal{I}_l(\pi(\mathbf{x}_j;\mathbf{p}_l))$: 像素点x在图像l上的重映射像素坐标
    - 共有L张图片，第一张作为target frame,剩下的L-1张是source frames
    - $M_P$: 对某点x做mask function后返回的$M_p$个visible points

  - 轮廓匹配误差silhouette matching error$\mathcal{L}_{cd}$
    $$
    \begin{align}
    &\mathcal{L}_{cd}(\mathbf{p}_0,\{\Delta\mathbf{p}_l\}^{L-1}_{l=1},s)=\\
    &\frac{1}{L}\sum^{L-1}_{l=0}(\sum_{\mathbf{u}_k\in U_{l\ 1}}\mathop{min}_{\mathbf{u}_j\in U_{l2}}||\mathbf{u}_k-\mathbf{u}_j||_2^2\\
    &+\sum_{\mathbf{u}_j\in U_{l\ 2}}\mathop{min}_{\mathbf{u}_k\in U_{l1}}||\mathbf{u}_j-\mathbf{u}_k||_2^2)
    \end{align} \tag{2}
    $$
    
  - 逆深度误差inverse depth error$\mathcal{L}_{invd}$
    $$
    \mathcal{L}_{invd}(\mathbf{p}_0,\{\Delta\mathbf{p}_l\}^{L-1}_{l=1},s)=\frac{1}{L}\sum_{l=0}^{L-1}\mathcal{L}_{\delta_2}(\mathbf{d}_l'-\alpha\mathbf{d}_l)\tag{3}
    $$
  
- 初始化

  style $\mathbf{s}$和pose $\mathbf{p}_0$初始化：

  - style：用cheap silhouette轮廓来得到style vector
  - pose：先用blender生成templates定一个相机位姿，然后找一个可以最大化IoU的模板

  Camera Motion参数初始化：
  $$
  \begin{cases}
  \Delta \mathcal{R}_0(w_0)\mathbf{x}+\mathbf{t}_0=\alpha(\mathbf{R'_0x+t'_0}) \\
  \Delta\mathcal{R}_l(\Delta w_l) \mathcal{R}_0(w_0)\mathbf{x}+\Delta\mathbf{t}_l+\mathbf{t}_0=\alpha(\mathbf{R'_l\mathbf{x}+\mathbf{t'_l}})
  \end{cases} \tag{4}
  $$
  
- 优化

  ![image-20230712105234987](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/202307121052041.png)

  - equ13：上面4式
  - equ8：上面3式的$\alpha$
  - [openMVS](https://github.com/cdcseacave/openMVS):open Multi-View Stereo reconstruction library

### 总结

SPBA对位姿和3D形状(由learned semantic shape prior而得)施加几何约束。

在世界坐标系中可以生成dense full 3D shape和深度图



## Object-Aware BA

- [论文](https://ora.ox.ac.uk/objects/uuid:580d2b5d-f8a0-44bb-bb8b-cd7518b97b16/download_file?safe_filename=icra.pdf.pdf&file_format=application%2Fpdf&type_of_work=Conference+item)

### 摘要

作者提出了一种单目方法，在点测量point measurements之外还进行对象检测object detection，**来消除尺度模糊性scale ambiguity和飘逸drift。**

通过对目标size的先验prior，可以将尺度估计scale estimation集成到ba中去。

本方法没有过去前端方法中constant camera height or planar roadways的限制，因此实用性更广泛

### 主要工作

1. 观察和世界模型

   ![image-20230712115729011](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/202307121157081.png)

   <center style="color:#C125C0C0">左：Parametrizations of points, objects and their respective projections. 右：Object detections and their labels.</center>

   - $Q_{kw}=[\mathbf{X},R]^T$：世界坐标系w下的物体k的位姿
     - $\mathbf{X}=[X,Y,Z,1]^T$
   - $\hat{q}_{ki}=[u,v,w,h]$：物体k在相机i坐标系下预测的映射
     - w,h是宽和高
   - $P_{kw}$：map point，看作是volume less的物体
   - $\hat{p}_{ki}=[u,v,0,0]$：因为无体积所以0，0

2. 物体测量和数据关联

   任何能够检测detect和关联associate输入序列input sequence的方法都可以。

   用来提供相机速度和尺度scale信息

3.  Object BA
   $$
   \mathop{arg\ min}_{\{\mathcal{T,P,Q}\}}(\sum_{i\in\mathcal{T}}\sum_{j\in\mathcal{P}}\mathbf{r}_{ij}^TW_{ij}^{-1}\mathbf{r}_{ij}+\sum_{i\in\mathcal{T}}\sum_{k\in\mathcal{Q}}\mathbf{r}_{ik}^TV_{ik}^{-1}\mathbf{r}_{ik}) \tag{1}
   $$

   - $\mathcal{T,P,Q}$: 位姿，路标点，物体
   - $r_{ij}=(\mathbf{p}_{ij}-\hat{\mathbf{p}}(T_i,\mathbf{P_{jw}}))$：路标和它的测量值之间的误差
     - 假设正态分布中心为0，协方差为$W$
   - $r_{ik}=(\mathbf{q}_{ik}-\hat{\mathbf{q}}(T_i,\mathbf{Q}_{kw}))$：物体和物体测量值之间的误差
     - 假设正态分布中心为0，协方差为$V$

4. 尺寸修正

   应用一个先验分布prior distribution，来确保地图的尺度和物体实际尺寸的一致。此时，物体的尺寸作为超参被使用，仅优化物体在三维空间中的位置。

   另外可以把路标点看作size为0的物体，这样1式可以化简为：
   $$
   \{\mathcal{Q,T}\}=\mathop{arg\ min}_{\{\mathcal{T,Q}\}}\sum_{i\in\mathcal{T}}\sum_{k\in\mathcal{Q}}\mathbf{r}_{ik}^TV_{ik}^{-1}\mathbf{r}_{ik} \tag{2}
   $$

5. 追踪和局部BA

   如果上面的全局BA计算量太大，所以考虑用10个关键帧算一个local BA：
   $$
   \{\mathcal{Q}_{local},\mathcal{T}_{local},T_{cam}\}=\mathop{arg\ min}_{\{\mathcal{Q}_{local},\mathcal{T}_{local},T_{cam}\}}\sum_{i}\sum_{k}\mathbf{r}_{ik}^TV_{ik}^{-1}\mathbf{r}_{ik} \tag{3}
   $$

   - $i\in{T_{cam},\mathcal{T}_{fixed},\mathcal{T}_{local}}$
     - $T_{cam}$：现在相机位姿
     - $\mathcal{T}_{local}$：最近的n=10个关键帧
     - $\mathcal{T}_{fixed}$：所有可以看到$\mathcal{Q}_{local}$的关键帧
   - $k\in\mathcal{Q}_{local}$
     - $\mathcal{Q}_{local}$：在这些关键帧可以看到的物体和点

6. 检测异常值

   使用robust error function on residuals来排除异常值，2式变成：
   $$
   \{\mathcal{Q,T}\}=\mathop{arg\ min}_{\{\mathcal{T,Q}\}}\sum_{i\in\mathcal{T}}\sum_{k\in\mathcal{Q}}Obj(|\mathbf{r}_{ik}|/V_{ik},\sigma_T) \tag{4}
   $$

   - $Obj(\cdot,\sigma_T)$：是Tukey biweight objective function
   - $\sigma_T$：在这里被设置为点误差分布(distribution of point errors)的估计标准偏差(estimated standard deviation)

### 总结

本方法把目标类object class的先验尺度信息整合到了单目slam中，却不增加计算复杂性。

本方法只假设存在一些已知的物体并且具有相对统一的尺寸，不需要其他假设。

系统能够在整个轨迹中保持较低的尺度漂移，并在长时间没有检测到物体的情况下纠正尺度漂移
