# BA结合神经网络

## BA-NET

- [论文](https://arxiv.org/abs/1806.04807)
- [git](https://github.com/frobelbest/BANet)

### 摘要

利用feature-metric(结合了特征点和度量信息的)BA创建的一种网络结构来处理SFM问题，

它们用feature-metric error的方式来实现multi-view geometry多视图几何约束。

### 主要工作

![image-20230708165631026](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/202307081656176.png)

<center style="color:#C125C0C0">BA-NET整体结构</center>

- 基本流程：
  1. 首先，DRN-54提取feature maps
  2. 然后，同时两件事：
     - Basis Depth Maps Generator对第一张图$I_1$生成多个基准深度图basis depth maps
     - Feature Pyramid Constructor对1中的feature maps提取出特征F 
  3. 最后，BA层用下面的1式同时优化相机位姿，深度

1. **特征金字塔结构**：

   ![image-20230708170036977](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/202307081700007.png)

   - 将骨干网络backbone network(蓝色)DRN-54(Dilated Residual Network-54)的最后几个残差快标记为C1,C2,C3,C4
     - 残差块（residual block）是深度残差网络（Residual Network）的基本组成单元。它包含一系列卷积层、激活函数和跳跃连接（skip connection）。
     - 跳跃连接将输入特征与残差块的输出特征进行直接相加，以便在信息传递过程中保留更多的低层细节和梯度信号。
     - 这些残差块通常是网络中的最后几个阶段，用于提取更高级别的特征表示，以便进行更复杂的任务，如语义理解或图像分割。
     - TIPS:本文用的DRN-54因为效率问题用ordinary convolution替代了dilation convolution。
   - 用Bilinear interpolation（双线性插值）对特征图$C^{k+1}$进行上采样，并和$C^k$结合后，用3X3的卷积层来将低维度到128维，最终得到图片$I_i$的特征$F_i$

2. **基准深度图**

  - Encoder: 用DRN-54来提取有用的特征
  - Decoder: 用卷积特征图作为基准深度图来优化

3. **BA层**：

   ![image-20230708183312273](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/202307081833302.png)

   - BA优化的error：
     $$
     e^f_{i,j}(\mathcal{X})=F_i(\pi(\mathbf{T}_i,d_j\cdot\mathbf{q}_j))-F_1(\mathbf{q_j})\tag{1}
     $$
     - $F_i$：是照片$I_i$的的特征金字塔
       - 特征金字塔：通过构建多尺度的特征表示，以捕捉图像中不同尺度的物体信息
     - $\mathcal{X}$：参数
       - $T_i$：i图下的相机位姿
       - $d_j$：j观测点(特征点)的深度
       - $q_j$：j观测点(特征点)的坐标
     
   - Figure4:

     - $E(\mathcal{X})=[e_{1,1}^f(\mathcal{X}),e_{1,2}^f(\mathcal{X})\cdots e_{N_i,N_j}^f(\mathcal{X})]$

       计算所有的$N_i$照片和$N_j$像素点的误差

     - 用global average pooling全局平均池化来聚合每个特征通道上所有像素的E(X)绝对值。

       然后用MLP网络预测damping factor$\lambda$。

     - 最后更新$\mathcal{X}$
       $$
       \begin{align}
       \Delta\mathcal{X}&=(J(\mathcal{X})^TJ(\mathcal{X})+\lambda D(\mathcal{X}))^{-1}J(\mathcal{X})^{T}E(\mathcal{X})
       \\
       \Delta\mathcal{X}&=g(\mathcal{X};\mathbb{F})
       \\
       \mathcal{X}_k&=g(\mathcal{X}_{k-1};\mathbb{F})\circ \mathcal{X}_{k-1}
       \end{align}
       $$

### 总结

网络模型 BA-NET利用特征度量误差(feature-metric error)来强制执行多视角几何约束。然后由feature-metric BA同时优化深度和位姿。

整个BA-Net的流程是可微分的，所以模型可以从数据中学习特征表示F，并且通过训练过程中的反向传播来更新模型参数$\lambda$

## DeepSFM

- [论文](https://arxiv.org/abs/1912.09697)
- [git](https://github.com/weixk2015/DeepSFM)

### 摘要

提出了一个名为DeepSFM的物理驱动架构，将深度学习和**显式的结构约束**(3D cost volume)相结合，来解决三维重建问题。

DeepSFM由两个基于成本体素的架构组成，分别用于深度估计和姿态估计，通过迭代运行来改进两者。

### 主要工作

![image-20230710000609230](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/202307100006281.png)

- 基本流程
  1. 使用2D CNN对输入图像进行处理，提取图像中的**光度特征**(photometric feature)
     - 光度特征可以是灰度图像中的像素值，也可以是彩色图像中的颜色通道值。
  2. 利用光度特征构建**成本体素**cost volume。
     - 成本体素是一种表示源图像和目标图像之间匹配程度的数据结构。
     - 它通过比较光度特征的相似性来计算匹配代价
  3. 初始源深度图(source depth maps)和相机位姿被用来引入**光度和几何一致性**(consistency)
     - 这些一致性约束有助于提高深度估计和姿态估计的准确性。
  4. 应用一系列的3D CNN层来处理**深度成本体素**（D-CV）和**位姿成本体素**（P-CV）。
     - 这些层用于进一步提取和学习特征，以优化深度和姿态的估计。
  5. 对于深度图，使用**上下文网络**(context network)和**深度回归操作**(depth regression operation)来生成目标图像的预测深度图。
     - 上下文网络可以考虑图像中的上下文信息
     - 深度回归操作用于预测每个像素点的深度值。

- **深度成本体素**(D-CV)

  区别于以前的成本体素，D-CV进一步利用由深度图带来的局部几何一致性约束(local geometric consistency constraints)。

  假设采样Hypothesis Sampling：

  - 为了将源视点的特征和深度图从源视角(source viewpoint)反投影(back-project)到目标视点的三维空间中，在逆深度空间(inverse-depth space)中均匀采样了一组L个**虚拟深度图平面**($d_l$)，
  - 这些平面与目标视点的前向方向（z轴）垂直。
  - 这些平面被用作输出深度图的假设，并且可以在它们之上构建成本体素。

  D-CV由三部分组成：

  - 目标图像特征target image features

  - 变换的源图特征warped source image features

    - 利用内参K和位姿T，将原图特征 $F$ 变换(warp)到每一个假设的深度图平面(hypothetical depth map planes $d_l$)
    - 再结合目标图像特征，得到一个$2Channel\times L\times Width \times Height$的特征量feature volume
      - 特征F的size: $Channel\times Width \times Height$
      - L：虚拟平面个数

  - 齐次深度一致性图homogeneous depth consistency maps

    为了利用几何一致性并提升深度预测的质量，我们在每个虚拟平面上增加了另外两个通道：

    1. 源视角的变换初始深度图(warped initial depth maps from the source views)

       和上面的变换的源图特征相同

    2. 以源视角为参考的投影虚拟深度平面(the projected virtual depth plane from the perspective of the source view)

       需要从目标视角到源视角进行坐标转换

- **位姿成本体素**(P-CV)

  ![image-20230710021541970](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/202307100215004.png)

  <center style="color:#C125C0C0">假设位姿采样</center>

  区别于D-CV是在假设深度图平面上创建，P-CV是在假设位姿上建立的，假设位姿采样如上图所示。a是对平移采样，b是对旋转采样。采样后就得到一组虚拟的位姿。

  P-CV也由三部分组成：

  - 目标图像特征
  - 变换的源图像特征
    - 通过双线性插值(bilinear sampling)对源视角的特征进行变换，得到变换后的源特征图
    - 结合目标图像特征和深度图，得到一个$(2Channel+2)\times P\times Width \times Height$的4D cost volume
      - W,H：特征图的宽和高
      - channel：通道数
      - P：采样的位姿个数
  - 齐次深度一致性图
    - 将初始目标视角深度(initial target view depth)和源视角(source view depth)深度转换为一个齐次坐标系
      - 这样可以在成本体素构建过程中增强 相机姿态 和 多视角深度图 之间的几何一致性


### 总结

DeepSFM深度学习框架通过两个关键组件D-CV和P-CV实现在深度网络中显式地强制执行光度一致性、几何一致性和相机运动约束。

可以看作是一种增强的基于学习的BA算法，它充分发挥了可学习的先验知识和几何规则的优势。



## Salient BA for VSLAM

- [论文](chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://arxiv.org/pdf/2012.11863.pdf)

### 摘要

旨在模仿人类视觉系统在不同任务下从自然场景中选择最显著和感兴趣的区域或点进行进一步处理。

用显著性预测模型(saliency prediction model)预测显著性地图(saliency map)，这个地图可以捕捉场景的语义的几何信息(scene semantic and geometric information)，然后这个地图的值作为传统BA中特征点的权重。

### 主要工作

![image-20230711005442714](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/202307110054860.png)

<center style="color:#C125C0C0">红色为本文贡献</center>

- 主要贡献：

  1. 提出了一个适用于室内和室外环境的 **Salient SLAM框架**，可以应用于各种应用领域。

     ![image-20230711010108808](https://raw.githubusercontent.com/Fernweh-yang/ImageHosting/main/img/202307110112364.png)

     <center style="color:#C125C0C0">Pipeline of visual saliency map prediction</center>

  2. 提出了一种新方法，基于KITTI生成**显著性数据集**Salient-KITTI(网上没找到)

     - 这个salient dataset包含几何和语义信息，用语义凝视(semantic gaze)来代替人眼凝视真值(human gaze ground-truth)
     - 之所以用这个数据集来训练，是因为之前依靠人眼追踪的gaze data无法捕捉到所有重要的信息。

  3. 用Salient-KITTI来做**显著性预测**

     1. 首先，对每个图像提取几何信息，如特征点、线条和平面
     2. 然后，使用SDC-Net在感兴趣物体周围生成分割掩码(segmented mask)
        - 作者选了13个类别（红绿灯、交通标志、道路、建筑物、人行道、停车场、铁轨、栅栏、桥梁、电线杆、电线杆组、植被、地形）来筛选几何信息
        - 动态区域中的特征不会出现在我们的显著性数据集中，如移动车辆、行人等
     3. 最后，基于我们提出的显著性数据集，使用DI-Net 获得一个显著性模型，并将其用于预测初始的显著性地图

  4. 提出了一种显著性**Salient Bundle Adjustment（SBA）**方法，以模拟人类视觉系统

     参考上面的总体流程图，SBA有两种：

     1. Motion-only BA: 用于优化位姿(R,t)，并最小化重映射误差
        $$
        \{R,t\}=\mathop{arg\ min}_{R,t}\sum_{i\in\mathcal{X}}\rho\big(w_i||x^i_{(\cdot)}-\pi_{(\cdot)}(\mathbf{RX})^i+t||^2_{\Sigma}\big)
        $$

        - $\rho$：robust Huber cost function

        - $w_i$:显著性权重salient weight
          $$
          w_i=aS^2(x_i,y_i)+b
          $$

          - S是显著性图Saliency map的像素值

     2. Local BA：用于优化共可见关键帧co-visible keyframe $\mathcal{K}_L$和这些关键帧中所有的地图点map-points $\mathcal{P}_L$
        $$
        \begin{align}
        &\{\mathbf{X}^i,\mathbf{R}_l,\mathbf{t}_l|i\in\mathcal{P_L},l\in\mathcal{K}_L \}=\mathop{arg\ min}_{\mathbf{X}^i,\mathbf{R}_l,\mathbf{t}_l}\sum_{k\in\mathcal{K}_L\cup\mathcal{K}_F}\sum_{j\in\mathcal{X}_k}\rho(E_{k,j})\\
        &E_{k,j}=w_i||x^j_{(\cdot)}-\pi_{(\cdot)}(\mathbf{R}_k\mathbf{X}^j+t_k)||^2_{\Sigma}
        \end{align}
        $$

        - X：地图点map-points $\mathcal{P}_L$和关键帧$\mathcal{K}_L$中key points相匹配的集合



### 总结

基于显著性信息，可以提高准确性和稳健性
